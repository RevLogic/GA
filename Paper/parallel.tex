\subsection{Parallelization}
Parallelization is an approach to computational problem-solving where the computation is divided into smaller sub-problems and the solution to each sub-problem is computed simultaneously. The solutions to each sub-problem are then combined to get the final result of the whole computation. The process of parallelizing a computation can be analyzed at different levels, from the bit level on a single machine to the distributed computing level over multiple machines (using cluster or grid computing).

\paragraph{Multiple Threading and Processes} Independent computation tasks may be delegated across separate processor cores using threads or processes. When processing large cascades, we can make use of these techniques in order to reduce computation time and take full advantage of the host system's processing capabilities.

\paragraph{Grid Computing}
In cases where instances of a problem are independent of each other (such as the block computations described under ``Parallelization in Revsim'' in the following section), parallelization is a useful method for reducing the amount of time required to compute a solution. Not only is it possible to distribute jobs (problem instances) across the processing units of a single machine, but using distributed computing systems, jobs may be delegated across an entire network of computers which work as distinct processing units. Such computing grids are powered by distributed software such as HTCondor (\cite{condor}). In typical grid scenarios, machine configurations (both in terms of hardware and software) are heterogeneous, thereby alleviating configuration-dependent hardware and software bugs. 